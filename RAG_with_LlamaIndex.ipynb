{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bcdb49ef9caa4200bbd6a32408e5f1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59b7c88751164801a320bbd7f947f583",
              "IPY_MODEL_e5fce247e38047208a8ad83d05e9f6aa",
              "IPY_MODEL_fd60d94d028040419cbab212057214bf"
            ],
            "layout": "IPY_MODEL_c0634a06f2d6439f9d2eca2d6e4bcf1e"
          }
        },
        "59b7c88751164801a320bbd7f947f583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1994aca118d640858ac12a86e1596b9e",
            "placeholder": "​",
            "style": "IPY_MODEL_6688c488b03042a3a4712a57a1974af9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e5fce247e38047208a8ad83d05e9f6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b767706c3807439690e176ad75a164cf",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_138b3cdfcbe74eba921e1e74920b68ad",
            "value": 3
          }
        },
        "fd60d94d028040419cbab212057214bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e21073ec5f242a0857b48131fa1d6af",
            "placeholder": "​",
            "style": "IPY_MODEL_599155209ff04ae1aba0bde0884252dd",
            "value": " 3/3 [01:18&lt;00:00, 26.02s/it]"
          }
        },
        "c0634a06f2d6439f9d2eca2d6e4bcf1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1994aca118d640858ac12a86e1596b9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6688c488b03042a3a4712a57a1974af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b767706c3807439690e176ad75a164cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "138b3cdfcbe74eba921e1e74920b68ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e21073ec5f242a0857b48131fa1d6af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "599155209ff04ae1aba0bde0884252dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "394cf6c152524a52bbab8f6c7630407f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17c1f7ebfcbe435099d69c0bc9146e9c",
              "IPY_MODEL_1f053f4d8ed14141ab51a02f06142be8",
              "IPY_MODEL_99a69a30b19d44c9839e23c398d5fc3a"
            ],
            "layout": "IPY_MODEL_a9674496274f4b0bac4c731f204ceceb"
          }
        },
        "17c1f7ebfcbe435099d69c0bc9146e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c38144b49672440ba0c80a6eb255b404",
            "placeholder": "​",
            "style": "IPY_MODEL_471e5f2e50884425a94b1a6af19831a1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1f053f4d8ed14141ab51a02f06142be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_586f7c83bc8242cebcc3f471fcd4b98b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f4262477ee648f4b4db1d26d9a3190c",
            "value": 2
          }
        },
        "99a69a30b19d44c9839e23c398d5fc3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_311fd9be5d1348a3bd0007787373ad23",
            "placeholder": "​",
            "style": "IPY_MODEL_74462bfa9e4041d193d207e14574d47e",
            "value": " 2/2 [01:23&lt;00:00, 38.11s/it]"
          }
        },
        "a9674496274f4b0bac4c731f204ceceb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c38144b49672440ba0c80a6eb255b404": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "471e5f2e50884425a94b1a6af19831a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "586f7c83bc8242cebcc3f471fcd4b98b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f4262477ee648f4b4db1d26d9a3190c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "311fd9be5d1348a3bd0007787373ad23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74462bfa9e4041d193d207e14574d47e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Table of content\n",
        "- [1. Retrieval Augmented Generation (RAG)](#1)\n",
        "    - [1.1 Stages within RAG](#1.1)\n",
        "    - [1.2 Components within RAG](#1.2)\n",
        "- [2. What is the LlamaIndex?](#2)\n",
        "- [3. Build a RAG System using LlamaIndex](#3)\n",
        "    - [3.1 Load Documents](#3.1)\n",
        "    - [3.2 Creating Text Chunks](#3.2)\n",
        "    - [3.2 Building Knowledge Bases](#3.3)\n",
        "    - [3.4 Query Index](#3.4)\n",
        "- [4. Build a RAG System with any LLM](#4)\n",
        "- [5. Build a RAG System from VinaLLaMA](#5)\n",
        "- [References](#6)\n",
        "\n",
        "**Note:** This notebook run on a single GPU - V100 16GB"
      ],
      "metadata": {
        "id": "cc0UkhBxVJjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "!pip install llama-index openai tiktoken pypdf accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKPd5FeIVJ9t",
        "outputId": "ae463906-e4d1-4f7c-c50e-b0115b4f05c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.9.23-py3-none-any.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.41.3.post2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.12.2 (from llama-index)\n",
            "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from llama-index)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
            "Collecting httpx (from llama-index)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.5.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions>=4.5.0 (from llama-index)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx->llama-index)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: bitsandbytes, typing-extensions, pypdf, mypy-extensions, marshmallow, h11, deprecated, beautifulsoup4, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, accelerate, openai, llama-index\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.25.0 beautifulsoup4-4.12.2 bitsandbytes-0.41.3.post2 dataclasses-json-0.6.3 deprecated-1.2.14 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 llama-index-0.9.23 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-1.6.1 pypdf-3.17.4 tiktoken-0.5.2 typing-extensions-4.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1' ></a>\n",
        "# 1. Retrieval Augmented Generation (RAG)\n",
        "LLMs undergo training on extensive datasets, excluding specific user data. Retrieval-Augmented Generation (RAG) tackles this limitation by dynamically integrating user data into the generation process. This is achieved without modifying the training data of LLMs; instead, the model gains access to and utilizes user data in real-time to offer more personalized and contextually appropriate responses.\n",
        "\n",
        "Within the RAG framework, user data is loaded and prepared for queries, essentially \"indexed.\" User queries interact with this index, refining the user data to the most pertinent context. The refined context and user query are then forwarded to the LLM, accompanied by a prompt, and the LLM generates a response.\n",
        "\n",
        "Whether you are constructing a chatbot or an agent, understanding RAG techniques for incorporating data into your application is essential.\n",
        "\n",
        "<a name='1.1' ></a>\n",
        "## 1.1 Stages within RAG\n",
        "\n",
        "![](https://i.imgur.com/JU101gO.png)\n",
        "\n",
        "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
        "\n",
        "- Loading: this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
        "- Indexing: this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
        "- Storing: Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\n",
        "- Querying: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies\n",
        "- Evaluation: a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\n",
        "\n",
        "<a name='1.2' ></a>\n",
        "## 1.2 Components within RAG\n",
        "In a typical RAG process, we have a few components.\n",
        "\n",
        "- Text Splitter: Splits documents to accommodate context windows of LLMs.\n",
        "- Embedding Model: The deep learning model used to get embeddings of documents.\n",
        "- Vector Stores: The databases where document embeddings are stored and queried along with their metadata.\n",
        "- LLM: The Large Language Model responsible for generating answers from queries.\n",
        "- Utility Functions: This involves additional utility functions such as Webretriver and document parsers that aid in retrieving and pre-processing files.\n"
      ],
      "metadata": {
        "id": "1qDn12RIzwR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='2' ></a>\n",
        "# 2. What is the LlamaIndex?\n",
        "\n",
        "LlamaIndex (formerly GPT Index), is a Python-based framework designed for constructing LLM applications. This framework serves as a straightforward and adaptable data solution, linking custom data sources to expansive language models. It offers specialized tools for seamless data ingestion from diverse sources, employs vector databases for efficient data indexing, and incorporates query interfaces tailored for handling extensive documents. In essence, The Llama Index stands as a comprehensive solution for developing retrieval augmented generation applications. Furthermore, it facilitates effortless integration with various applications such as Langchain, Flask, Docker, and more. For additional details, please visit the official GitHub repository at [https://github.com/run-llama/llama_index](https://github.com/run-llama/llama_index)."
      ],
      "metadata": {
        "id": "sAltqEJR6BpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='3' ></a>\n",
        "# 3. Build a RAG System using LlamaIndex"
      ],
      "metadata": {
        "id": "wD9DFTOP7HdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.text_splitter import TokenTextSplitter\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index import set_global_service_context\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"YOUR_OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "o5HfcjMM9gXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='3.1' ></a>\n",
        "## 3.1 Load Documents\n",
        "As we know, LLMs lack updated knowledge of the world and information about internal documents. To enhance the capabilities of LLMs, it is necessary to provide them with pertinent information sourced from knowledge repositories. These repositories may comprise structured data like CSV, Spreadsheets, or SQL tables, unstructured data such as texts, Word Docs, Google Docs, PDFs, or PPTs, and semi-structured data like Notion, Slack, Salesforce, etc.\n",
        "\n",
        "This notebook focuses on utilizing PDFs as knowledge sources. The Llama Index incorporates a class called SimpleDirectoryReader, designed to read stored documents from a specified directory. It automatically chooses a parser based on the file extension for efficient processing.\n",
        "\n",
        "In the below code, we use a RAG pipeline system to question and answering on ebook [`How to Build a Career in AI`](https://wordpress.deeplearning.ai/wp-content/uploads/2022/10/eBook-How-to-Build-a-Career-in-AI.pdf)"
      ],
      "metadata": {
        "id": "_Rn9Be_4_sZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[\"./eBook-How-to-Build-a-Career-in-AI.pdf\"]\n",
        ").load_data()"
      ],
      "metadata": {
        "id": "st1v_GZA-tWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(documents), \"\\n\")\n",
        "print(len(documents), \"\\n\")\n",
        "print(type(documents[0]))\n",
        "print(documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGEGtkCD_A95",
        "outputId": "575d9b5d-a740-4531-eee6-56069cc6691c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> \n",
            "\n",
            "41 \n",
            "\n",
            "<class 'llama_index.schema.Document'>\n",
            "Doc ID: 85fae675-6f1d-4232-a371-a0ccdbc2ce7f\n",
            "Text: PAGE 1Founder, DeepLearning.AICollected Insights from Andrew Ng\n",
            "How to  Build Your Career in AIA Simple Guide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='3.2' ></a>\n",
        "## 3.2 Creating Text Chunks\n",
        "Frequently, data extracted from knowledge sources surpasses the context window of LLMs. When texts longer than the context window are transmitted, the ChatGPT API trims the data, leading to the exclusion of essential information. Text chunking presents a solution to this challenge, wherein longer texts are divided into smaller chunks based on separators.\n",
        "\n",
        "Apart from facilitating the fitting of texts into the context window of large language models, text chunking offers additional advantages:\n",
        "\n",
        "- Enhanced embedding accuracy: Smaller text chunks contribute to improved embedding accuracy, subsequently elevating retrieval accuracy.\n",
        "- Precision in context: Refining information through text chunking enhances the accuracy of the context, leading to better retrieval of information.\n",
        "\n",
        "The Llama Index incorporates built-in tools specifically designed for text chunking. Here is the process of implementing text chunking using the Llama Index."
      ],
      "metadata": {
        "id": "k4HSDvHO_-Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "node_parser = SimpleNodeParser.from_defaults(\n",
        "  separator=\" \",\n",
        "  chunk_size=1024,\n",
        "  chunk_overlap=20,\n",
        "  tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "U0x22rAY_Lnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='3.3' ></a>\n",
        "## 3.3 Building Knowledge Bases\n",
        "The texts extracted from the knowledge sources need to be stored somewhere. But in RAG-based applications, we need the embeddings of the data. These embeddings are floating point numbers representing data in a high-dimensional vector space. To store and operate on them, we need vector databases. Vector Databases are purpose-built data stores for storing and querying vectors."
      ],
      "metadata": {
        "id": "HeilyBZ9CF5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings\n",
        "llm = OpenAI(model='gpt-3.5-turbo', temperature=0.7, max_tokens=256)\n",
        "embed_model = OpenAIEmbedding()\n",
        "\n",
        "prompt_helper = PromptHelper(\n",
        "  context_window=4096,\n",
        "  num_output=256,\n",
        "  chunk_overlap_ratio=0.1,\n",
        "  chunk_size_limit=None\n",
        ")\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "  llm=llm,\n",
        "  embed_model=embed_model,\n",
        "  node_parser=node_parser,\n",
        "  prompt_helper=prompt_helper\n",
        ")"
      ],
      "metadata": {
        "id": "FujbShL6A3Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector Database\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "ZWyQrfyoCfco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='3.4' ></a>\n",
        "## 3.4 Query Index\n",
        "The final step is to query from the index and get a response from the LLM. Llama Index provides a query engine for querying and a chat engine for a chat-like conversation. The difference between the two is the chat engine preserves the history of the conversation, and the query engine does not."
      ],
      "metadata": {
        "id": "sd7ou8aLCrt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(service_context=service_context)\n",
        "response = query_engine.query(\"What are steps to take when finding projects to build your experience?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfyCIJXSCpS5",
        "outputId": "bfd56e04-09b0-4d8a-eda1-92b633dfacef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider the technical growth potential of the project and ensure it is challenging but not too difficult. Also, assess whether there are good teammates or people to discuss ideas with, as collaborators can greatly impact your growth. Additionally, determine if the project can act as a stepping stone to larger projects based on its technical complexity and business impact. Finally, avoid spending excessive time on project selection and instead focus on taking action and refining your thinking as you work on multiple projects throughout your career.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "OK307OjPDHjD",
        "outputId": "f9d8c140-03fa-48e0-baec-5a06f73316cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Consider the technical growth potential of the project and ensure it is challenging but not too difficult. Also, assess whether there are good teammates or people to discuss ideas with, as collaborators can greatly impact your growth. Additionally, determine if the project can act as a stepping stone to larger projects based on its technical complexity and business impact. Finally, avoid spending excessive time on project selection and instead focus on taking action and refining your thinking as you work on multiple projects throughout your career.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4' ></a>\n",
        "# 4. Build a RAG System with any LLM\n",
        "\n",
        "LlamaIndex supports using LLMs from HuggingFace directly. Note that for a completely private experience, also setup a local embeddings model.\n",
        "\n",
        "Many open-source models from HuggingFace require either some preamble before each prompt, which is a `system_prompt`. Additionally, queries themselves may need an additional wrapper around the `query_str` itself. All this information is usually available from the HuggingFace model card for the model you are using."
      ],
      "metadata": {
        "id": "9IhduhojDmf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "model_name = \"berkeley-nest/Starling-LM-7B-alpha\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"cuda\")\n",
        "system_prompt = \"\"\"System: You are usefull LLM to build a RAG System.<|end_of_turn|>\"\"\"\n",
        "\n",
        "# This will wrap the default prompts that are internal to llama-index\n",
        "query_wrapper_prompt = PromptTemplate(\"User:{query_str} <|end_of_turn|>\\nAssistant: \")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=model_name,\n",
        "    model_name=model_name,\n",
        "    device_map=\"cuda\",\n",
        "    stopping_ids=[tokenizer.eos_token_id],\n",
        "    tokenizer_kwargs={\"max_length\": 4096},\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16}\n",
        ")\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "bcdb49ef9caa4200bbd6a32408e5f1c6",
            "59b7c88751164801a320bbd7f947f583",
            "e5fce247e38047208a8ad83d05e9f6aa",
            "fd60d94d028040419cbab212057214bf",
            "c0634a06f2d6439f9d2eca2d6e4bcf1e",
            "1994aca118d640858ac12a86e1596b9e",
            "6688c488b03042a3a4712a57a1974af9",
            "b767706c3807439690e176ad75a164cf",
            "138b3cdfcbe74eba921e1e74920b68ad",
            "3e21073ec5f242a0857b48131fa1d6af",
            "599155209ff04ae1aba0bde0884252dd"
          ]
        },
        "id": "uk0S8FEbDQ_q",
        "outputId": "899fab14-77da-4419-bbbb-8ecadcc5f353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcdb49ef9caa4200bbd6a32408e5f1c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    service_context=service_context\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(service_context=service_context)\n",
        "response = query_engine.query(\"What are steps to take when finding projects to build your experience?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE9wVRPIIHGq",
        "outputId": "dc5f6c67-1d6b-4a71-8209-83092d329d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are some steps to take when finding projects to build your experience:\n",
            "\n",
            "1. **Identify your interests and goals**: Think about what topics or industries you're passionate about and what career goals you have. This will help you find projects that align with your interests and goals.\n",
            "\n",
            "2. **Research different types of projects**: Look for projects in your areas of interest that span various levels of difficulty and scope. Some projects might be part of a class or a competition, while others could be personal or professional side projects. Consider the potential impact and technical complexity of each project.\n",
            "\n",
            "3. **Look for opportunities to collaborate**: Working with a team can help you learn from others and develop your skills more effectively. Connect with people who share your interests and goals, and consider joining a club, attending workshops, or participating in online forums to find potential collaborators.\n",
            "\n",
            "4. **Evaluate the potential of each project**: Before committing to a project, consider whether it can help you grow technically, whether you have a good team to work with, and if it could be a stepping stone to larger projects. It's also important to choose projects that are challenging enough to stretch your skills but not too difficult that you have little chance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='5' ></a>\n",
        "# 5. Build a RAG System from VinaLLaMA\n",
        "In this part, we use a RAG pipeline system built from `VinaLLaMA - State-of-the-art Vietnamese LLMs` to question and answering on document[`AI tạo sinh: Sức bật giúp doanh nghiệp Việt Nam về đích tăng trưởng`](https://vinbigdata.com/document)"
      ],
      "metadata": {
        "id": "X9JzmtqvKkBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: You need to restart kernel to avoid OutOfMemoryError before loading VinaLLaMA\n",
        "import os\n",
        "from llama_index import ServiceContext\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"YOUR_OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "o4jH7yEBkCtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[\"./23127_VBDI_Ebook-AI-tao-sinh-Final.pdf\"]\n",
        ").load_data()"
      ],
      "metadata": {
        "id": "eU5XPb4kQNTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(documents), \"\\n\")\n",
        "print(len(documents), \"\\n\")\n",
        "print(type(documents[20]))\n",
        "print(documents[20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgu_AgxrRb_Y",
        "outputId": "3726a744-bfdd-48eb-936e-6b5e244005c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> \n",
            "\n",
            "24 \n",
            "\n",
            "<class 'llama_index.schema.Document'>\n",
            "Doc ID: 4e4d6223-968b-4274-a1c7-9497bc06e2c9\n",
            "Text: PHẦN 5 VINBIGDATA/colon.uc TIÊN PHONG PHÁT TRIỂN MÔ HÌNH NGÔN\n",
            "NGỮ LỚN TIẾNG VIỆT PHẦN 5 /hyphen.uc TIÊN PHONG PHÁT TRIỂN MÔ HÌNH\n",
            "NGÔN NGỮ LỚN TIẾNG VIỆT 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "model_name = \"vilm/vinallama-7b-chat\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"cuda\")\n",
        "system_prompt = \"\"\"<|im_start|>system\n",
        "Bạn là một trợ lí AI hữu ích. Hãy trả lời người dùng một cách chính xác.\n",
        "<|im_end|>\"\"\"\n",
        "\n",
        "# This will wrap the default prompts that are internal to llama-index\n",
        "query_wrapper_prompt = PromptTemplate(\"<|im_start|>user\\n{query_str} <|im_end|>\\n<|im_start|>assistant\")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=model_name,\n",
        "    model_name=model_name,\n",
        "    device_map=\"cuda\",\n",
        "    stopping_ids=[tokenizer.eos_token_id],\n",
        "    tokenizer_kwargs={\"max_length\": 4096},\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16}\n",
        ")\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    # Uncomment this if using a embedding model on local\n",
        "    # embed_model=\"local\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "394cf6c152524a52bbab8f6c7630407f",
            "17c1f7ebfcbe435099d69c0bc9146e9c",
            "1f053f4d8ed14141ab51a02f06142be8",
            "99a69a30b19d44c9839e23c398d5fc3a",
            "a9674496274f4b0bac4c731f204ceceb",
            "c38144b49672440ba0c80a6eb255b404",
            "471e5f2e50884425a94b1a6af19831a1",
            "586f7c83bc8242cebcc3f471fcd4b98b",
            "4f4262477ee648f4b4db1d26d9a3190c",
            "311fd9be5d1348a3bd0007787373ad23",
            "74462bfa9e4041d193d207e14574d47e"
          ]
        },
        "id": "QVHd3h2iKzAR",
        "outputId": "c1d64f54-cf93-4223-a818-4383ff11879d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "394cf6c152524a52bbab8f6c7630407f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    service_context=service_context\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(service_context=service_context)\n",
        "response = query_engine.query(\"Làm thế nào để lựa chọn LLM phù hợp?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2I6RamjPVQU",
        "outputId": "06383ead-3840-4900-a7b9-228caa819cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Để lựa chọn LLM phù hợp, doanh nghiệp nên xem xét các tiêu chí sau và cân nhắc chúng dựa trên chiến lược và chính sách của họ:\n",
            "\n",
            "1. Hiệu suất hoặc chi phí triển khai: Tùy thuộc vào nhu cầu cụ thể của doanh nghiệp, ưu tiên các tiêu chí này hơn tiêu chí khác.\n",
            "2. Mô hình công nghệ: Chọn mô hình phù hợp nhất với nhu cầu của doanh nghiệp và chiến lược kinh doanh của họ.\n",
            "3. Bảo mật dữ liệu: Tập trung vào các mô hình do nước ngoài phát triển có dữ liệu lưu trữ tại các máy chủ bên ngoài Việt Nam hoặc sử dụng các dịch vụ đám mây, vì điều này có thể tạo ra nguy cơ mất dữ liệu và xâm phạm quyền riêng tư.\n",
            "4. Tính chính xác của thông tin mang tính bản địa: Chọn các mô hình sử dụng nguồn dữ liệu mang tính bản địa cao, vì điều này sẽ đảm bảo mô hình trả về phản hồi phù hợp với bối cảnh văn hóa, kinh tế và xã hội của Việt Nam.\n",
            "5. Ngân sách và lợi suất dự kiến: Hiểu rõ ngân sách của doanh nghiệp và lợi suất dự kiến, và lựa chọn mô hình phù hợp nhất với những yếu tố này.\n",
            "6. Phản hồi chính xác: Tập trung\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='6' ></a>\n",
        "# References\n",
        "- [Evaluate RAG with LlamaIndex](https://cookbook.openai.com/examples/evaluation/evaluate_rag_with_llamaindex)\n",
        "- [Build a RAG Pipeline With the LLama Index](https://www.analyticsvidhya.com/blog/2023/10/rag-pipeline-with-the-llama-index/)\n",
        "- [Customizing LLMs within LlamaIndex Abstractions](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom.html)\n",
        "- [RAG with LlamaIndex and DeciLM: A Step-by-Step Tutorial](https://deci.ai/blog/rag-with-llamaindex-and-decilm-a-step-by-step-tutorial/)\n",
        "\n",
        "See more detail at my github - [QuyAnh2005](https://github.com/QuyAnh2005/RAG-LlamaIndex)"
      ],
      "metadata": {
        "id": "cMP04F4zUGXr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFnJVCYcBf9E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}